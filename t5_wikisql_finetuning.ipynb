{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVcjK2c5sqVR"
      },
      "outputs": [],
      "source": [
        "%pip install datasets\n",
        "%pip install transformers[torch]\n",
        "%pip install pytorch_lightning\n",
        "%pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTIPBLV0z8qm"
      },
      "outputs": [],
      "source": [
        "# For Google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/Colab Notebooks/nlp/t5_trained_model\"\n",
        "tokenizer_path = \"/content/drive/MyDrive/Colab Notebooks/nlp/t5_tokenizer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g3_ior5spr3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "train_dataset = load_dataset('wikisql', split='train') # 56,355 samples\n",
        "valid_dataset = load_dataset('wikisql', split='validation') # 8,421 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sua_J9esspr5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "class WikiSQLDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data, max_len_inp=512,max_len_out=96):\n",
        "        self.answer = \"answer\"\n",
        "        self.question = \"question\"\n",
        "        self.data = data #demo purposes\n",
        "\n",
        "        self.max_len_input = max_len_inp\n",
        "        self.max_len_output = max_len_out\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "        #squeeze to get rid of the batch dimension\n",
        "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # convert [batch,dim] to [dim]\n",
        "\n",
        "        labels = copy.deepcopy(target_ids)\n",
        "        labels [labels==0] = -100\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask,\n",
        "                \"target_ids\": target_ids, \"target_mask\": target_mask,\n",
        "                \"labels\":labels}\n",
        "\n",
        "    def _build(self):\n",
        "        for row in self.data:\n",
        "            question = row['question']\n",
        "            question = f\"translate English to SQL: {question}\"\n",
        "            target = row['sql']['human_readable']\n",
        "\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [question], max_length=self.max_len_input,\n",
        "                truncation = True,\n",
        "                padding='max_length', return_tensors=\"pt\"\n",
        "            )\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length=self.max_len_output,\n",
        "                truncation = True,\n",
        "                padding='max_length',return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFYTgXnzspr7"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "      T5ForConditionalGeneration,\n",
        "      T5Tokenizer\n",
        "  )\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base',model_max_length=512)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "train_data = WikiSQLDataset(t5_tokenizer, train_dataset)\n",
        "validation_data = WikiSQLDataset(t5_tokenizer, valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSTyQs_cspr9"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class T5Tuner(pl.LightningModule):\n",
        "    def __init__(self,batchsize, t5model, t5tokenizer):\n",
        "        super(T5Tuner, self).__init__()\n",
        "        self.batch_size = batchsize\n",
        "        self.model = t5model\n",
        "        self.tokenizer = t5tokenizer\n",
        "\n",
        "\n",
        "    def forward( self, input_ids, attention_mask=None,\n",
        "                decoder_attention_mask=None,\n",
        "                lm_labels=None):\n",
        "\n",
        "         outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "\n",
        "         return outputs\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            lm_labels=batch['labels']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        self.log('train_loss',loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            lm_labels=batch['labels']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        self.log(\"val_loss\",loss)\n",
        "        return loss\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(train_data, batch_size=self.batch_size,\n",
        "                          num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(validation_data,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=2)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=3e-4, eps=1e-8)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9lLF2IRspr-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "# Model Fine-Tuning\n",
        "bs = 8\n",
        "model = T5Tuner(bs, t5_model, t5_tokenizer)\n",
        "ckpt_path_dir = \"/content/drive/MyDrive/Colab Notebooks/nlp/t5-checkpoint\"\n",
        "trainer = pl.Trainer(max_epochs = 3, accelerator=device, default_root_dir=ckpt_path_dir, enable_checkpointing=True)\n",
        "trainer.fit(model)\n",
        "\n",
        "\n",
        "#save artifacts for deployment and inference\n",
        "model.model.save_pretrained(train_path)\n",
        "t5_tokenizer.save_pretrained(tokenizer_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
